{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuiOu8-vYK2H",
        "outputId": "c86d483d-a54d-47cb-a6d8-d6059753bf3e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AGMAcS5vVfo",
        "outputId": "22264723-f8e5-4485-cc43-73db8bdda6bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jYtcNgiVvrIM"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'root': \".\"\n",
        "    , 'train_path':  '/content/drive/MyDrive/lg_aimers/data/train.csv'  #\"./train.csv\"\n",
        "    , 'submit_path': '/content/drive/MyDrive/lg_aimers/data/submission.csv' #\"./submission.csv\"\n",
        "    , 'seed_list': [42, 137] # , 56, 89, 24, 75 ,88 ,36 ,71]\n",
        "    , 'k_fold': 5\n",
        "    , 'thresholds': {'product_category': 10, 'expected_timeline': 3}\n",
        "}\n",
        "\n",
        "cbt_params = {\n",
        "    'random_seed': config['seed_list'],\n",
        "    'objective': 'Logloss',\n",
        "    'auto_class_weights': 'Balanced',\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "\n",
        "tuning_params = [\n",
        "    {\n",
        "        'learning_rate': 0.05\n",
        "        , 'n_estimators': 3000\n",
        "\n",
        "        , 'early_stopping_rounds': 50\n",
        "\n",
        "        # regularizations\n",
        "        , 'max_depth': 6\n",
        "        , 'l2_leaf_reg': 1\n",
        "        , 'min_data_in_leaf': 2\n",
        "        , 'subsample': 0.5\n",
        "        # ,'grow_policy': 'Depthwise' # 'SymmetricTree'(default)\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C3B6q6GVvL8P"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "hYCMK9UMZ_Yd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EELu-r_cwOPh"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    # Set the seed for reproducibility.\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    # torch.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    # torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    # torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "\n",
        "\n",
        "def read_data(config):\n",
        "    # Load training, testing, and submission CSV files\n",
        "    df_train = pd.read_csv(config['train_path'])  # 학습용 데이터\n",
        "    df_test = pd.read_csv(config['submit_path'])  # 테스트 데이터(제출파일의 데이터)\n",
        "    df_sub = pd.read_csv(config['submit_path'])\n",
        "\n",
        "    return df_train, df_test, df_sub\n",
        "\n",
        "\n",
        "def get_clf_eval(y_test, y_pred=None, fold_no=None):\n",
        "    # Calculate and print evaluation metrics and confusion matrix,\n",
        "    # accuracy, precision, recall, and F1 score.\n",
        "    # Optionally includes fold number in the output.\n",
        "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    F1 = f1_score(y_test, y_pred, labels=[True, False])\n",
        "\n",
        "    fold_info = f'Fold #{fold_no}' if fold_no is not None else ''\n",
        "    print(f'{fold_info} ACC: {accuracy:.4f}, PRE: {precision:.4f}, REC: {recall:.4f}, F1: {F1:.4f}')\n",
        "    return F1\n",
        "\n",
        "\n",
        "def new_business_area(cur_area):\n",
        "    # Categorizes current business area into a new, simplified business area group.\n",
        "    # Returns the original area if it doesn't match the above categories.\n",
        "    if cur_area in ['corporate / office', 'government department']:\n",
        "        return 'Office'\n",
        "    elif cur_area in ['education', 'public facility']:\n",
        "        return 'Public'\n",
        "    elif cur_area in ['hotel & accommodation', 'residential (home)']:\n",
        "        return 'Amenity'\n",
        "    elif cur_area in ['factory', 'power plant / renewable energy', 'transportation']:\n",
        "        return 'Industry'\n",
        "    else:\n",
        "        return cur_area\n",
        "\n",
        "\n",
        "def make_value_count(value_count_dict, val):\n",
        "     # Returns the count of occurrences of val from a precomputed dictionary\n",
        "     # otherwise returns NaN.\n",
        "    if val in value_count_dict:\n",
        "        return value_count_dict[val]\n",
        "    return np.NAN\n",
        "\n",
        "\n",
        "def make_value_count_dict(df, col_names):\n",
        "    # Creates a dictionary of value counts for each feature name\n",
        "    value_count_dict = dict()\n",
        "    for feat_name in col_names:\n",
        "        total_count = df[feat_name].value_counts()\n",
        "\n",
        "        # Converts the Series object to a DataFrame and then to a dictionary.\n",
        "        count_df = pd.DataFrame(total_count).reset_index(drop=False)\n",
        "\n",
        "        value_count_dict[feat_name] = dict(zip(count_df.iloc[:, 0], count_df.iloc[:, 1]))\n",
        "\n",
        "    return value_count_dict\n",
        "\n",
        "\n",
        "def make_continent(rc):\n",
        "    # Maps response corporate (rc) codes to their respective continent.\n",
        "    # Each if statement checks for membership in a list of codes corresponding to a continent.\n",
        "    if rc in ('lgein', 'lgeml', 'lgeph', 'lgeth', 'lgevh', 'lgeil', 'lgekr', 'lgett', 'lgejp', 'lgech', 'lgeir', 'lgesj',\n",
        "              'lgegf', 'lgetk', 'lgelf', 'lgehk', 'lgeyk'):\n",
        "        return 'asia'\n",
        "    if rc in ('lgeaf', 'lgesa', 'lgemc', 'lgeas', 'lgeeg', 'lgeef'):\n",
        "        return 'africa'\n",
        "    if rc in ('lgeus', 'lgeci'):\n",
        "        return 'northamerica'\n",
        "    if rc in ('lgesp', 'lgecb', 'lgems', 'lgecl', 'lgeps', 'lgear', 'lgepr'):\n",
        "        return 'southamerica'\n",
        "    if rc in ('lgeuk', 'lgees', 'lgefs', 'lgebn', 'lgebt', 'lgedg', 'lgero', 'lgemk', 'lgepl', 'lgecz', 'lgehs', 'lgesw',\n",
        "              'lgeag', 'lgeeb', 'lgera', 'lgeur', 'lgept', 'lgeis', 'lgela'):\n",
        "        return 'europe'\n",
        "    if rc in ('lgesl', 'lgeap'):\n",
        "        return 'oceania'\n",
        "    return np.NaN\n",
        "\n",
        "\n",
        "def feature_engineering(df_input, is_train=False):\n",
        "    df = df_input.copy()\n",
        "\n",
        "    # drop_duplicates\n",
        "    df = df.drop_duplicates(keep='first')\n",
        "\n",
        "    # converting text to lowercase, removing commas and periods\n",
        "    # and replacing underscores and hyphens with spaces.\n",
        "    for feat in df.columns:\n",
        "        # Applies transformations only to string-type columns.\n",
        "        if df[feat].dtype == 'object':\n",
        "            df[feat] = df[feat].str.lower()\n",
        "            df[feat] = df[feat].str.replace('[,\\.]', '', regex=True)\n",
        "            df[feat] = df[feat].str.replace('[_-]', ' ', regex=True)\n",
        "\n",
        "    # Recategorizes 'Solution' and 'CM' in 'business_unit' column to 'Others'.\n",
        "    df['business_unit'] = np.where(df['business_unit'].isin(['Solution', 'CM']), 'Others', df['business_unit'])\n",
        "\n",
        "    # map ['other', 'others', 'etc'] -> 'Others'\n",
        "    unify_others_columns = ['customer_type', 'customer_job', 'inquiry_type', 'product_category', 'product_subcategory',\n",
        "                            'customer_position', 'expected_timeline']\n",
        "    for column in unify_others_columns:\n",
        "        df[column] = df[column].replace(['other', 'others', 'etc'], 'others')\n",
        "\n",
        "    # make new columns\n",
        "    df['continent'] = df['response_corporate'].map(make_continent)\n",
        "    df['business_area_group'] = df['business_area'].map(new_business_area)\n",
        "    df['product_category_count'] = df['product_category'].apply(\n",
        "        lambda x: len(str(x).split(',')) if not pd.isna(x) else np.nan)\n",
        "\n",
        "    make_value_count_columns = ['lead_owner']\n",
        "    if is_train:\n",
        "        global value_count_dict\n",
        "        value_count_dict = make_value_count_dict(df_input, make_value_count_columns)\n",
        "\n",
        "    for feat_name in value_count_dict:\n",
        "        func = partial(make_value_count, value_count_dict[feat_name])\n",
        "        df[f'{feat_name}_count'] = df[feat_name].map(func)\n",
        "\n",
        "    # correct data type\n",
        "    for feat in ['customer_idx', 'lead_owner', 'ver_cus', 'ver_pro']:\n",
        "        df[feat] = df[feat].astype(object)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_feature_lists(df):\n",
        "    base_features = []     # all features except target variable.\n",
        "    base_num_features = [] # numerical features\n",
        "    base_cat_features = [] # categorical features\n",
        "\n",
        "    for feat in df.columns:\n",
        "        # skip the target\n",
        "        if feat == 'is_converted':\n",
        "            continue\n",
        "\n",
        "        base_features.append(feat)\n",
        "\n",
        "        if df[feat].dtype == 'object':\n",
        "            base_cat_features.append(feat)\n",
        "        else:\n",
        "            base_num_features.append(feat)\n",
        "\n",
        "    # features to be removed from data analysis\n",
        "    removal_features = {\n",
        "        'id', 'bant_submit', 'id_strategic_ver', 'it_strategic_ver', 'idit_strategic_ver',\n",
        "        'customer_country', 'customer_country.1', 'business_subarea', 'business_area'\n",
        "    }\n",
        "\n",
        "    # remove the specified features\n",
        "    base_num_features = [i for i in base_num_features if i not in removal_features]\n",
        "    base_cat_features = [i for i in base_cat_features if i not in removal_features]\n",
        "    base_features = [i for i in base_features if i not in removal_features]\n",
        "\n",
        "    return base_num_features, base_cat_features, base_features\n",
        "\n",
        "\n",
        "def filling_missing_values(df_input, base_cat_features, base_num_features):\n",
        "    df = df_input.copy()\n",
        "\n",
        "    # Fill missing values for categorical features with 'UNK'\n",
        "    # and ensure their data type is string.\n",
        "    for base_cat_feat in base_cat_features:\n",
        "        df[base_cat_feat] = df[base_cat_feat].fillna('UNK')\n",
        "        df[base_cat_feat] = df[base_cat_feat].astype(str)\n",
        "\n",
        "    # Fill missing values for numerical features with -1.\n",
        "    for base_num_feat in base_num_features:\n",
        "        df[base_num_feat] = df[base_num_feat].fillna(-1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def model_kfold(df, config, cbt_params, base_features, base_cat_features):\n",
        "    target = 'is_converted'\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=config['k_fold'], shuffle=True, random_state=config['seed'])\n",
        "    cbt_models = [] # trained models\n",
        "    f1_scores = []  # f1-scores for validation sets\n",
        "\n",
        "    for k_fold, (train_idx, valid_idx) in enumerate(skf.split(df[base_features], df[target])):\n",
        "        print(f'Fold #{k_fold + 1}')\n",
        "        X_train, y_train = df[base_features].iloc[train_idx], df[target].iloc[train_idx].astype(int)\n",
        "        X_valid, y_valid = df[base_features].iloc[valid_idx], df[target].iloc[valid_idx].astype(int)\n",
        "\n",
        "        # initialize CatBoost model with provided parameters\n",
        "        cbt = CatBoostClassifier(**cbt_params)\n",
        "\n",
        "        cbt.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_valid, y_valid)],\n",
        "            cat_features=base_cat_features, # specify categorical features\n",
        "        )\n",
        "\n",
        "        # save the trained model\n",
        "        cbt_models.append(cbt)\n",
        "\n",
        "        # evaluate the model\n",
        "        # --- train-set\n",
        "        print('[Train] ', end='')\n",
        "        y_pred = cbt.predict(X_train)\n",
        "        _ = get_clf_eval(y_train, y_pred, k_fold + 1)\n",
        "\n",
        "        # --- valid-set\n",
        "        print('[Valid] ', end='')\n",
        "        y_pred = cbt.predict(X_valid)\n",
        "        y_pred = y_pred.astype(y_valid.dtype) # ensure matching dtype\n",
        "        f1 = get_clf_eval(y_valid, y_pred, k_fold + 1)\n",
        "\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    print(f'Avg. F1 of validset: {np.mean(f1_scores)}')\n",
        "    print(f'Var. F1 of validset: {np.var(f1_scores)}')\n",
        "\n",
        "    return cbt_models\n",
        "\n",
        "\n",
        "def kfold_submission(df_train, df_test, df_sub, cbt_models, config):\n",
        "    folder_path = f\"{config['root']}/FeatureImportance\"\n",
        "\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # get current date and time\n",
        "    now = datetime.now()\n",
        "\n",
        "    # record the year, month, day, hour, and minute for naming files.\n",
        "    year = now.year\n",
        "    month = now.month\n",
        "    day = now.day\n",
        "    hour = now.hour\n",
        "    minute = now.minute\n",
        "\n",
        "    # file format\n",
        "    submission_time = f\"{year:04d}{month:02d}{day:02d}_{hour:02d}{minute:02d}\"[2:]\n",
        "    target = 'is_converted'\n",
        "\n",
        "    # apply feature engineering\n",
        "    df_test = feature_engineering(df_test, is_train=False)\n",
        "    base_num_features, base_cat_features, base_features = make_feature_lists(df_test)\n",
        "\n",
        "    df_test = filling_missing_values(df_test, base_cat_features, base_num_features)\n",
        "\n",
        "    X_test = df_test[base_features]\n",
        "\n",
        "    # a matrix to store the probabilities of each class for the test set.\n",
        "    y_probs = np.zeros((X_test.shape[0], 2))\n",
        "\n",
        "    # dataframe for feature importances\n",
        "    df_feature_importance_all = pd.DataFrame({'features': base_features})\n",
        "\n",
        "    for i, cbt_model in enumerate(cbt_models):\n",
        "        y_probs += cbt_model.predict_proba(X_test) / len(cbt_models)\n",
        "\n",
        "        # save feature importance of current model\n",
        "        df_feature_importance_all[f'model_{i}'] = cbt_model.get_feature_importance()\n",
        "\n",
        "    # compute avarege, rank\n",
        "    df_feature_importance_all['average'] = df_feature_importance_all.iloc[:, 1:].mean(axis=1).values\n",
        "    df_feature_importance_all['rank'] = df_feature_importance_all['average'].rank(ascending=False)\n",
        "\n",
        "    # save the feature importance as CSV\n",
        "    df_feature_importance_all.to_csv(f'{folder_path}/feat_import_{submission_time}.csv', index=False)\n",
        "\n",
        "    # create submission file\n",
        "    # threshold: 0.5 to determine the class\n",
        "    df_sub[target] = (y_probs[:, 1] >= 0.5).astype(bool)\n",
        "\n",
        "    # save submission file as CSV\n",
        "    df_sub.to_csv(f\"{config['root']}/submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jfb2XSQLwFPn"
      },
      "outputs": [],
      "source": [
        "def main(config, cbt_params, tuning_params):\n",
        "    cbt_models = []\n",
        "\n",
        "    for seed in config['seed_list']:\n",
        "        for tuning_param in tuning_params:\n",
        "            config['seed'] = seed\n",
        "            cbt_params['random_seed'] = seed\n",
        "\n",
        "            for param in cbt_params:\n",
        "                tuning_param[param] = cbt_params[param]\n",
        "            print(tuning_param)\n",
        "\n",
        "            # seed 설정\n",
        "            set_seed(config['seed'])\n",
        "\n",
        "            # 데이터 셋 읽어오기\n",
        "            df_train, df_test, df_sub = read_data(config)\n",
        "\n",
        "            # 데이터 전처리\n",
        "            df_train = feature_engineering(df_train, is_train=True)\n",
        "            base_num_features, base_cat_features, base_features = make_feature_lists(df_train)\n",
        "\n",
        "            # 결측치 채우기\n",
        "            df_train = filling_missing_values(df_train, base_cat_features, base_num_features)\n",
        "\n",
        "            # 모델 성능 확인(train, valid 데이터 활용)\n",
        "            cbt_model = model_kfold(df_train, config, tuning_param, base_features, base_cat_features)\n",
        "\n",
        "            cbt_models.extend(cbt_model)\n",
        "\n",
        "    # 제출하기\n",
        "    kfold_submission(df_train, df_test, df_sub, cbt_models, config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_kfold_submission(df_train, df_test, df_sub, lgb_models, config):\n",
        "    folder_path = f\"{config['root']}/FeatureImportance\"\n",
        "\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # get current date and time\n",
        "    now = datetime.now()\n",
        "\n",
        "    # record the year, month, day, hour, and minute for naming files.\n",
        "    year = now.year\n",
        "    month = now.month\n",
        "    day = now.day\n",
        "    hour = now.hour\n",
        "    minute = now.minute\n",
        "\n",
        "    # file format\n",
        "    submission_time = f\"{year:04d}{month:02d}{day:02d}_{hour:02d}{minute:02d}\"[2:]\n",
        "    target = 'is_converted'\n",
        "\n",
        "    # apply feature engineering\n",
        "    df_test = feature_engineering(df_test, is_train=False)\n",
        "    base_num_features, base_cat_features, base_features = make_feature_lists(df_test)\n",
        "\n",
        "    df_test = filling_missing_values(df_test, base_cat_features, base_num_features)\n",
        "\n",
        "    X_test = df_test[base_features].copy()\n",
        "    for cat_feat in base_cat_features:\n",
        "        X_test[cat_feat] = X_test[cat_feat].astype('category')\n",
        "        #X_test[cat_feat] = X_test[cat_feat].map(lambda s: config['le_dict'][cat_feat].transform([s])[0] if s in config['le_dict'][cat_feat].classes_ else -1)\n",
        "\n",
        "    # a matrix to store the probabilities of each class for the test set.\n",
        "    y_probs = np.zeros((X_test.shape[0], 2))\n",
        "\n",
        "    # dataframe for feature importances\n",
        "    df_feature_importance_all = pd.DataFrame({'features': base_features})\n",
        "\n",
        "    for i, lgb_model in enumerate(lgb_models):\n",
        "        y_probs += lgb_model.predict_proba(X_test) / len(lgb_models)\n",
        "\n",
        "        # save feature importance of current model\n",
        "        df_feature_importance_all[f'model_{i}'] = lgb_model.booster_.feature_importance(importance_type='gain')\n",
        "\n",
        "    # compute avarege, rank\n",
        "    df_feature_importance_all['average'] = df_feature_importance_all.iloc[:, 1:].mean(axis=1).values\n",
        "    df_feature_importance_all['rank'] = df_feature_importance_all['average'].rank(ascending=False)\n",
        "\n",
        "    # save the feature importance as CSV\n",
        "    df_feature_importance_all.to_csv(f'{folder_path}/feat_import_{submission_time}.csv', index=False)\n",
        "\n",
        "    # create submission file\n",
        "    # threshold: 0.5 to determine the class\n",
        "    df_sub[target] = (y_probs[:, 1] >= 0.5).astype(bool)\n",
        "\n",
        "    # save submission file as CSV\n",
        "    df_sub.to_csv(f\"{config['root']}/submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "9hy-eDQP_jbQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_model_kfold(df, config, lgb_params, base_features, base_cat_features):\n",
        "    target = 'is_converted'\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=config['k_fold'], shuffle=True, random_state=config['seed'])\n",
        "    lgb_models = [] # trained models\n",
        "    f1_scores = []  # f1-scores for validation sets\n",
        "\n",
        "    for k_fold, (train_idx, valid_idx) in enumerate(skf.split(df[base_features], df[target])):\n",
        "        print(f'Fold #{k_fold + 1}')\n",
        "        X_train, y_train = df[base_features].iloc[train_idx], df[target].iloc[train_idx].astype(int)\n",
        "        X_valid, y_valid = df[base_features].iloc[valid_idx], df[target].iloc[valid_idx].astype(int)\n",
        "\n",
        "        # initialize CatBoost model with provided parameters\n",
        "        lgb_clf = lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "        # lgb_train = lgb.Dataset(X_train, label=y_train, categorical_feature =base_cat_features)\n",
        "        # lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
        "\n",
        "        # lgb_clf = lgb.train(\n",
        "        #     lgb_params,\n",
        "        #     lgb_train,\n",
        "        #     valid_sets=[lgb_valid],\n",
        "        # )\n",
        "\n",
        "        lgb_clf.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_valid, y_valid)],\n",
        "            categorical_feature =base_cat_features, # specify categorical features\n",
        "        )\n",
        "\n",
        "        # save the trained model\n",
        "        lgb_models.append(lgb_clf)\n",
        "\n",
        "        # evaluate the model\n",
        "        # --- train-set\n",
        "        print('[Train] ', end='')\n",
        "        y_pred = lgb_clf.predict(X_train)\n",
        "        _ = get_clf_eval(y_train, y_pred, k_fold + 1)\n",
        "\n",
        "        # --- valid-set\n",
        "        print('[Valid] ', end='')\n",
        "        y_pred = lgb_clf.predict(X_valid)\n",
        "        y_pred = y_pred.astype(y_valid.dtype) # ensure matching dtype\n",
        "        f1 = get_clf_eval(y_valid, y_pred, k_fold + 1)\n",
        "\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    print(f'Avg. F1 of validset: {np.mean(f1_scores)}')\n",
        "    print(f'Var. F1 of validset: {np.var(f1_scores)}')\n",
        "\n",
        "    return lgb_models"
      ],
      "metadata": {
        "id": "2t1UgqDqbjvv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_lgb(config, lgb_params, lgb_tuning_params):\n",
        "    lgb_models = []\n",
        "\n",
        "    for seed in config['seed_list']:\n",
        "        for tuning_param in lgb_tuning_params:\n",
        "            config['seed'] = seed\n",
        "            lgb_params['random_seed'] = seed\n",
        "\n",
        "            for param in lgb_params:\n",
        "                tuning_param[param] = lgb_params[param]\n",
        "            print('tuning_param', tuning_param)\n",
        "\n",
        "            # seed 설정\n",
        "            set_seed(config['seed'])\n",
        "\n",
        "            # 데이터 셋 읽어오기\n",
        "            df_train, df_test, df_sub = read_data(config)\n",
        "\n",
        "            # 데이터 전처리\n",
        "            df_train = feature_engineering(df_train, is_train=True)\n",
        "            base_num_features, base_cat_features, base_features = make_feature_lists(df_train)\n",
        "\n",
        "            # 결측치 채우기\n",
        "            df_train = filling_missing_values(df_train, base_cat_features, base_num_features)\n",
        "\n",
        "            # LGBM을 위한 전처리: label encoding\n",
        "            le_dict = {}\n",
        "            for cat_feat in base_cat_features:\n",
        "                df_train[cat_feat] = df_train[cat_feat].astype('category')\n",
        "            #     le = LabelEncoder()\n",
        "            #     df_train[cat_feat] = le.fit_transform(df_train[cat_feat])\n",
        "            #     le_dict[cat_feat] = le\n",
        "            # config['le_dict'] = le_dict\n",
        "\n",
        "            # 모델 성능 확인(train, valid 데이터 활용)\n",
        "            lgb_model = lgb_model_kfold(df_train, config, tuning_param, base_features, base_cat_features)\n",
        "\n",
        "            lgb_models.extend(lgb_model)\n",
        "\n",
        "    # 제출하기\n",
        "    lgb_kfold_submission(df_train, df_test, df_sub, lgb_models, config)"
      ],
      "metadata": {
        "id": "IRZZ6vBfez_B"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_params = {\n",
        "    'random_seed': config['seed_list'],\n",
        "    'objective': 'binary',\n",
        "    'is_unbalance': 'true',\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "lgb_tuning_params = [\n",
        "    {\n",
        "        'learning_rate': 0.05,\n",
        "        'n_estimators': 3000,\n",
        "        'early_stopping_round': 50,\n",
        "        'max_depth': -1,\n",
        "        #'num_leaves': 80,\n",
        "        'reg_lambda': 1,\n",
        "        'subsample': 0.5,\n",
        "        #'n_jobs': -1,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "WHFL-z8dbjz_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_lgb(config, lgb_params, lgb_tuning_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OhxjzHjfZ-3",
        "outputId": "c7b362b8-4ed1-4998-ce10-8bf5bcb83954"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuning_param {'learning_rate': 0.05, 'n_estimators': 3000, 'early_stopping_round': 50, 'max_depth': -1, 'reg_lambda': 1, 'subsample': 0.5, 'random_seed': 42, 'objective': 'binary', 'is_unbalance': 'true', 'verbose': 0}\n",
            "Fold #1\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #1 ACC: 0.9881, PRE: 0.8746, REC: 1.0000, F1: 0.9331\n",
            "[Valid] Fold #1 ACC: 0.9682, PRE: 0.7676, REC: 0.8831, F1: 0.8213\n",
            "Fold #2\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #2 ACC: 0.9910, PRE: 0.9021, REC: 0.9995, F1: 0.9483\n",
            "[Valid] Fold #2 ACC: 0.9699, PRE: 0.7940, REC: 0.8593, F1: 0.8254\n",
            "Fold #3\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #3 ACC: 0.9866, PRE: 0.8607, REC: 0.9997, F1: 0.9250\n",
            "[Valid] Fold #3 ACC: 0.9675, PRE: 0.7624, REC: 0.8820, F1: 0.8179\n",
            "Fold #4\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #4 ACC: 0.9891, PRE: 0.8838, REC: 0.9997, F1: 0.9382\n",
            "[Valid] Fold #4 ACC: 0.9668, PRE: 0.7633, REC: 0.8690, F1: 0.8128\n",
            "Fold #5\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #5 ACC: 0.9871, PRE: 0.8655, REC: 0.9995, F1: 0.9277\n",
            "[Valid] Fold #5 ACC: 0.9696, PRE: 0.7832, REC: 0.8755, F1: 0.8268\n",
            "Avg. F1 of validset: 0.8208187518467576\n",
            "Var. F1 of validset: 2.6041615547458325e-05\n",
            "tuning_param {'learning_rate': 0.05, 'n_estimators': 3000, 'early_stopping_round': 50, 'max_depth': -1, 'reg_lambda': 1, 'subsample': 0.5, 'random_seed': 137, 'objective': 'binary', 'is_unbalance': 'true', 'verbose': 0}\n",
            "Fold #1\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #1 ACC: 0.9900, PRE: 0.8923, REC: 0.9997, F1: 0.9430\n",
            "[Valid] Fold #1 ACC: 0.9688, PRE: 0.7791, REC: 0.8701, F1: 0.8221\n",
            "Fold #2\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #2 ACC: 0.9882, PRE: 0.8752, REC: 1.0000, F1: 0.9335\n",
            "[Valid] Fold #2 ACC: 0.9661, PRE: 0.7551, REC: 0.8745, F1: 0.8104\n",
            "Fold #3\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #3 ACC: 0.9878, PRE: 0.8713, REC: 1.0000, F1: 0.9312\n",
            "[Valid] Fold #3 ACC: 0.9685, PRE: 0.7790, REC: 0.8658, F1: 0.8201\n",
            "Fold #4\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #4 ACC: 0.9900, PRE: 0.8929, REC: 0.9995, F1: 0.9432\n",
            "[Valid] Fold #4 ACC: 0.9702, PRE: 0.7863, REC: 0.8799, F1: 0.8304\n",
            "Fold #5\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[Train] Fold #5 ACC: 0.9880, PRE: 0.8737, REC: 0.9997, F1: 0.9325\n",
            "[Valid] Fold #5 ACC: 0.9675, PRE: 0.7710, REC: 0.8636, F1: 0.8147\n",
            "Avg. F1 of validset: 0.8195500091566703\n",
            "Var. F1 of validset: 4.639198181534076e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze_w1TZBytke"
      },
      "outputs": [],
      "source": [
        "#main(config, cbt_params, tuning_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ildJgr82yyai"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}